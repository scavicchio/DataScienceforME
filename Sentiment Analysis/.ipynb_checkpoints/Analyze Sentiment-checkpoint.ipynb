{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://programminghistorian.org/en/lessons/sentiment-analysis\n",
    "# https://textblob.readthedocs.io/en/dev/quickstart.html#spelling-correction\n",
    "#https://www.kaggle.com/ssishu/factual-authenticity-analysis-of-tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Text</th>\n",
       "      <th>user</th>\n",
       "      <th>retweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-21 23:59:43+00:00</td>\n",
       "      <td>It would be interesting to see if the Wuhan co...</td>\n",
       "      <td>eugenegu</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-21 23:59:41+00:00</td>\n",
       "      <td>CBP Enacts ‘Enhanced Health Screening’ as Chin...</td>\n",
       "      <td>BreitbartTexas</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-21 23:59:06+00:00</td>\n",
       "      <td>Chinese coronavirus may have been lurking in a...</td>\n",
       "      <td>MailOnline</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2020-01-21 23:58:27+00:00</td>\n",
       "      <td>CDC and WHO are learning more about this new c...</td>\n",
       "      <td>NYCHealthCommr</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2020-01-21 23:54:52+00:00</td>\n",
       "      <td>My take: China is doing a massive cover up &amp; p...</td>\n",
       "      <td>IndoPac_Info</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                   Datetime  \\\n",
       "0           0  2020-01-21 23:59:43+00:00   \n",
       "1           1  2020-01-21 23:59:41+00:00   \n",
       "2           2  2020-01-21 23:59:06+00:00   \n",
       "3           3  2020-01-21 23:58:27+00:00   \n",
       "4           4  2020-01-21 23:54:52+00:00   \n",
       "\n",
       "                                                Text            user  retweets  \n",
       "0  It would be interesting to see if the Wuhan co...        eugenegu        87  \n",
       "1  CBP Enacts ‘Enhanced Health Screening’ as Chin...  BreitbartTexas        57  \n",
       "2  Chinese coronavirus may have been lurking in a...      MailOnline        23  \n",
       "3  CDC and WHO are learning more about this new c...  NYCHealthCommr        25  \n",
       "4  My take: China is doing a massive cover up & p...    IndoPac_Info        23  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the 3 tweets \n",
    "import pandas as pd\n",
    "df_gen = pd.read_csv(\"../Raw Tweets/general_covid_tweet_data.csv\")\n",
    "df_nyc_covid = pd.read_csv(\"../Raw Tweets/nyc_covid_tweet_data.csv\")\n",
    "df_nyc_gen = pd.read_csv(\"../Raw Tweets/nyc_popular_tweet_data.csv\")\n",
    "\n",
    "df_gen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It would be interesting to see if the Wuhan coronavirus binds to the Angiotensin-converting enzyme 2 (ACE2) receptor just like SARS to invade our cells. If so, we can potentially use ACE2 as a competitive inhibitor to treat these coronavirus infections in hypertensive patients.\n",
      "\n",
      "CBP Enacts ‘Enhanced Health Screening’ as Chinese Coronavirus Comes to U.S. http://bit.ly/2TOCJ7J via @BreitbartTexas\n",
      "\n",
      "Chinese coronavirus may have been lurking in animals for DECADES \n",
      "\n",
      "CDC and WHO are learning more about this new coronavirus every day. At this time, the actual risk to New Yorkers is low, but our level of preparedness is high. We are monitoring the situation daily. \n",
      "\n",
      "My take: China is doing a massive cover up & practicing criminal negligence. Lots of people are going to the hospitals with symptoms, but they are not tested for #coronavirus, they get diagnosed with \"viral pneumonia\", get left to die, quickly cremated &amp; are not counted in stats\n"
     ]
    }
   ],
   "source": [
    "# cleaning columns I don't think we will need\n",
    "df_gen.drop(columns=['Unnamed: 0','user']);\n",
    "df_nyc_covid.drop(columns=['Unnamed: 0','user']);\n",
    "df_nyc_gen.drop(columns=['Unnamed: 0','user'])\n",
    "\n",
    "print(df_gen[\"Text\"][0])\n",
    "print()\n",
    "print(df_gen[\"Text\"][1])\n",
    "print()\n",
    "print(df_gen[\"Text\"][2])\n",
    "print()\n",
    "print(df_gen[\"Text\"][3])\n",
    "print()\n",
    "print(df_gen[\"Text\"][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of common contractions from wikipedia\n",
    "contractions = { \n",
    "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is / how does\",\n",
    "\"I'd\": \"I had / I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I shall / I will\",\n",
    "\"I'll've\": \"I shall have / I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now I want to do some text cleaning in the actual data\n",
    "# these are some helper functions that I will use later\n",
    "import string\n",
    "def removePuncuation(text):\n",
    "    # need to do this as would rather have two words than a single\n",
    "    # word that is supposed to be hyphenated\n",
    "    text = text.replace('-',' ')\n",
    "    return \"\".join([x for x in text if x not in string.punctuation])\n",
    "\n",
    "def removeMentions(text):\n",
    "    return\n",
    "    \n",
    "def removeLinks(text):\n",
    "    return\n",
    "\n",
    "def makeLowercase(text):\n",
    "    return \" \".join(x.lower() for x in x.split())\n",
    "    \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "def removeStopwords(text): \n",
    "    return [x for x in text if x not in stopwords.words('english')]\n",
    "\n",
    "from textblob import TextBlob\n",
    "def correctSpelling(text): \n",
    "    return TextBlob(text).correct() # about 70% accurate\n",
    "\n",
    "from pycontractions import Contractions\n",
    "cont = Contractions('GoogleNews-vectors-negative300.bin')\n",
    "def expandContractions(text): \n",
    "    return cont.expand_text(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It would be interesting to see if the Wuhan coronavirus binds to the Angiotensin converting enzyme 2 ACE2 receptor just like SARS to invade our cells If so we can potentially use ACE2 as a competitive inhibitor to treat these coronavirus infections in hypertensive patients\n",
      "\n",
      "CBP Enacts ‘Enhanced Health Screening’ as Chinese Coronavirus Comes to US httpbitly2TOCJ7J via BreitbartTexas\n",
      "\n",
      "Chinese coronavirus may have been lurking in animals for DECADES \n",
      "\n",
      "CDC and WHO are learning more about this new coronavirus every day At this time the actual risk to New Yorkers is low but our level of preparedness is high We are monitoring the situation daily \n",
      "\n",
      "My take China is doing a massive cover up  practicing criminal negligence Lots of people are going to the hospitals with symptoms but they are not tested for coronavirus they get diagnosed with viral pneumonia get left to die quickly cremated amp are not counted in stats\n",
      "       Unnamed: 0                   Datetime  \\\n",
      "0               0  2020-01-21 23:59:43+00:00   \n",
      "1               1  2020-01-21 23:59:41+00:00   \n",
      "2               2  2020-01-21 23:59:06+00:00   \n",
      "3               3  2020-01-21 23:58:27+00:00   \n",
      "4               4  2020-01-21 23:54:52+00:00   \n",
      "...           ...                        ...   \n",
      "10395          95  2020-05-03 22:57:11+00:00   \n",
      "10396          96  2020-05-03 22:53:25+00:00   \n",
      "10397          97  2020-05-03 22:51:44+00:00   \n",
      "10398          98  2020-05-03 22:50:38+00:00   \n",
      "10399          99  2020-05-03 22:50:26+00:00   \n",
      "\n",
      "                                                    Text            user  \\\n",
      "0      It would be interesting to see if the Wuhan co...        eugenegu   \n",
      "1      CBP Enacts ‘Enhanced Health Screening’ as Chin...  BreitbartTexas   \n",
      "2      Chinese coronavirus may have been lurking in a...      MailOnline   \n",
      "3      CDC and WHO are learning more about this new c...  NYCHealthCommr   \n",
      "4      My take China is doing a massive cover up  pra...    IndoPac_Info   \n",
      "...                                                  ...             ...   \n",
      "10395  Piers Morgan temporarily steps back from Good ...         BBCNews   \n",
      "10396    JAEHYUN we dont stan coronavirus in this house         vousyeon   \n",
      "10397  Hoarding Supplieshttpswwwreplytonewscom2020050...    ReplyToNews1   \n",
      "10398             it’s called sarcasm world coronavirus      stopandpaws   \n",
      "10399  President Trump is seeing support slide among ...     WSJPolitics   \n",
      "\n",
      "       retweets  \n",
      "0            87  \n",
      "1            57  \n",
      "2            23  \n",
      "3            25  \n",
      "4            23  \n",
      "...         ...  \n",
      "10395       151  \n",
      "10396         3  \n",
      "10397         0  \n",
      "10398         0  \n",
      "10399       271  \n",
      "\n",
      "[10400 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# apply the cleaning functions \n",
    "\n",
    "df_gen['Text'] = df_gen['Text'].apply(lambda x : removePuncuation(x))\n",
    "\n",
    "print(df_gen[\"Text\"][0])\n",
    "print()\n",
    "print(df_gen[\"Text\"][1])\n",
    "print()\n",
    "print(df_gen[\"Text\"][2])\n",
    "print()\n",
    "print(df_gen[\"Text\"][3])\n",
    "print()\n",
    "print(df_gen[\"Text\"][4])\n",
    "print(df_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download(\"all\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.tag import pos_tag_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
